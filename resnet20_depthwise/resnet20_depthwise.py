# -*- coding: utf-8 -*-
"""resnet20_depthwise.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jvKdve3z01-Tl_MvCFeMDNYV9crTGjVw
"""

import torch
import torch.nn as nn
import torch.nn.functional as F

def depthwise_conv3x3(in_channels,out_channels,stride=1):
  return nn.Conv2d(in_channels,out_channels,kernel_size=3,stride = stride,padding =1,bias=False,groups = in_channels)

def pointwise_conv(in_channels,out_channels,stride=1):
  return nn.Conv2d(in_channels,out_channels,kernel_size=1,stride = stride,bias=False)

class depthwise_separable(nn.Module):
  def __init__(self, in_channels, out_channels, stride = 1):
    super(depthwise_separable, self).__init__()

    self.conv1 = depthwise_conv3x3(in_channels,out_channels,stride=stride)
    self.bn1 = nn.BatchNorm2d(out_channels)
    self.relu = nn.ReLU(inplace=True)
    self.conv2 = pointwise_conv(out_channels,out_channels,stride=1)
    self.bn2 = nn.BatchNorm2d(out_channels)

  def forward(self, x):
    x = self.conv1(x)
    x = self.bn1(x)
    x = self.relu(x)
    x = self.conv2(x)
    x = self.bn2(x)
    x = self.relu(x)
    return x


class basic_block(nn.Module):
  def __init__(self,in_channels,out_channels,stride=1):
    super(basic_block,self).__init__()
    self.conv1 = depthwise_separable(in_channels,out_channels,stride=stride)
    self.conv2 = depthwise_separable(out_channels,out_channels,stride=1)
    if in_channels != out_channels:
      self.downsample = identity_mapping(in_channels,out_channels,stride)
    else:
      self.downsample = None

  def forward(self,x):
    shortcut = x
    x = self.conv1(x)
    x = self.conv2(x)
    if self.downsample is not None:
      shortcut = self.downsample(shortcut)
    x = x + shortcut
    return x

class identity_mapping(nn.Module):
  def __init__(self, in_channels, out_channels, stride):
    super(identity_mapping, self).__init__()

    self.pooling = nn.MaxPool2d(1, stride=stride)
    self.add_channels = out_channels - in_channels

  def forward(self, x):
    out = F.pad(x, (0, 0, 0, 0, 0, self.add_channels))
    out = self.pooling(out)
    return out

class ResNet_cifar10(nn.Module):
  def __init__(self,block,num_classes,N):
    super(ResNet_cifar10,self).__init__()
    self.conv1 = nn.Conv2d(3,16,kernel_size = 3,padding =1,bias=False)
    self.bn1 = nn.BatchNorm2d(16)
    self.relu = nn.ReLU(inplace=True)
    self.conv2 = self._make_group(block,16,16,N)
    self.conv3 = self._make_group(block,16,32,N,stride = 2)
    self.conv4 = self._make_group(block,32,64,N,stride = 2)
    self.avg_pool = nn.AdaptiveAvgPool2d((1,1))
    self.fc = nn.Linear(64, num_classes)

  def _make_group(self,block,in_channels,out_channels,N,stride = 1):
    group = []
    group.append(block(in_channels,out_channels,stride = stride))
    for _ in range(1,N):
      group.append(block(out_channels,out_channels))
    return nn.Sequential(*group)

  def forward(self,x):
    x = self.conv1(x)
    x = self.bn1(x)
    x = self.relu(x)
    x = self.conv2(x)
    x = self.conv3(x)
    x = self.conv4(x)
    x = self.avg_pool(x)
    x = torch.flatten(x, 1)
    x = self.fc(x)
    return x

def resnet20_cifar10(num_classes=10):
  return ResNet_cifar10(basic_block, num_classes, 3)

import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim import lr_scheduler
import torch.backends.cudnn as cudnn

import torchvision
import torchvision.transforms as transforms
from torchvision.datasets import CIFAR10
from torch.utils.data import DataLoader

device = 'cuda' if torch.cuda.is_available() else 'cpu'

print('==> Preparing data..')
transforms_train = transforms.Compose([
	transforms.RandomCrop(32, padding=4),
	transforms.RandomHorizontalFlip(),
	transforms.ToTensor(),
	transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
])
transforms_test = transforms.Compose([
	transforms.ToTensor(),
	transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
])

dataset_train = CIFAR10(root='./data', train=True, 
						download=True, transform=transforms_train)
dataset_test = CIFAR10(root='./data', train=False, 
					   download=True, transform=transforms_test)
train_loader = DataLoader(dataset_train, batch_size=128, 
	                      shuffle=True)
test_loader = DataLoader(dataset_test, batch_size=128, 
	                     shuffle=False)

# Commented out IPython magic to ensure Python compatibility.
from torch.utils.tensorboard import SummaryWriter
# %load_ext tensorboard
writer = SummaryWriter()

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard  --logdir=runs

# there are 10 classes so the dataset name is cifar-10
classes = ('plane', 'car', 'bird', 'cat', 'deer', 
	       'dog', 'frog', 'horse', 'ship', 'truck')

print('==> Making model..')

net = resnet20_cifar10()
net = net.to(device)
num_params = sum(p.numel() for p in net.parameters() if p.requires_grad)
print('The number of parameters of model is', num_params)

criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.1, 
	                  momentum=0.9, weight_decay=1e-4)

decay_epoch = [32000, 48000]
step_lr_scheduler = lr_scheduler.MultiStepLR(optimizer, 
								 milestones=decay_epoch, gamma=0.1)


def train(epoch, global_steps):
	net.train()

	train_loss = 0
	correct = 0
	total = 0

	for batch_idx, (inputs, targets) in enumerate(train_loader):
		global_steps += 1
		step_lr_scheduler.step()
		inputs = inputs.to(device)
		targets = targets.to(device)
		outputs = net(inputs)
		loss = criterion(outputs, targets)

		optimizer.zero_grad()
		loss.backward()
		optimizer.step()

		train_loss += loss.item()
		_, predicted = outputs.max(1)
		total += targets.size(0)
		correct += predicted.eq(targets).sum().item()
		
	acc = 100 * correct / total
	print('train epoch : {} [{}/{}]| loss: {:.3f} | acc: {:.3f}'.format(
		   epoch, batch_idx, len(train_loader), train_loss/(batch_idx+1), acc))
	writer.add_scalar('log/train error', 100 - acc, global_steps)
	return global_steps


def test(epoch, best_acc, global_steps):
	net.eval()

	test_loss = 0
	correct = 0
	total = 0

	with torch.no_grad():
		for batch_idx, (inputs, targets) in enumerate(test_loader):
			inputs = inputs.to(device)
			targets = targets.to(device)
			outputs = net(inputs)
			loss = criterion(outputs, targets)

			test_loss += loss.item()
			_, predicted = outputs.max(1)
			total += targets.size(0)
			correct += predicted.eq(targets).sum().item()
	
	acc = 100 * correct / total
	print('test epoch : {} [{}/{}]| loss: {:.3f} | acc: {:.3f}'.format(epoch, batch_idx, len(test_loader), test_loss/(batch_idx+1), acc))
	writer.add_scalar('log/test error', 100 - acc, global_steps)
    
	if acc > best_acc:
		print('==> Saving model..')
		state = {
		    'net': net.state_dict(),
		    'acc': acc,
		    'epoch': epoch,
		}
		if not os.path.isdir('save_model'):
		    os.mkdir('save_model')
		torch.save(state, './save_model/ckpt.pth')
		best_acc = acc

	return best_acc


if __name__=='__main__':
  best_acc = 0
  epoch = 0
  global_steps = 0
  while True:
    epoch += 1
    global_steps = train(epoch, global_steps)
    best_acc = test(epoch, best_acc, global_steps)
    print('best test accuracy is ', best_acc)
    if global_steps >= 64000:
      break
